---
layout: default
title: Awesome Jailbreaking Academy Resources
---

![Awesome Badge](https://awesome.re/badge-flat.svg)
# Awesome Jailbreaking Academy Resources

**A curated list of high-quality resources for learning and practicing jailbreaking—the art of circumventing or extending the limits of AI systems, especially large language models.** Each link in this list should provide clear value and context for learners and practitioners.

The goal of this repository is twofold: to provide a rigorous knowledge base for the community and to serve as a collaborative platform to enrich that base over time. The content is short, curated, and should point to high-quality resources.

## Contents

- [Guides and Tutorials](#guides-and-tutorials)
- [Tools and Libraries](#tools-and-libraries)
- [Communities and Forums](#communities-and-forums)
- [Articles and Publications](#articles-and-publications)
- [Videos and Presentations](#videos-and-presentations)
- [Open‑Source Resources](#opensource-resources)

## Guides and Tutorials

- [OpenAI Usage Policies](https://openai.com/policies/usage-policies) - Official guidance describing acceptable and disallowed uses of deployed models.
- [Model Cards for Model Reporting](https://modelcards.withgoogle.com/about) - Guidance for documenting model capabilities, limitations, and evaluation practices.

## Tools and Libraries

- [Robustness Gym (Airbnb)](https://github.com/airbnb/robustness-gym) - Toolkit for measuring and comparing model robustness across datasets and transformations.
- [TextAttack](https://github.com/QData/TextAttack) - Framework for adversarial attacks, data augmentation, and model evaluation for NLP.
- [Hugging Face Datasets](https://huggingface.co/datasets) - Collection of datasets useful for evaluation, stress-testing, and robustness experiments.

## Communities and Forums

- [AI Alignment Forum](https://www.alignmentforum.org) - Research discussions focused on long-term AI safety, evaluation, and robustness.
- [LessWrong](https://www.lesswrong.com) - Community posts and essays covering ML risks, model behavior, and safety discussions.

## Articles and Publications

- [Adversarial Examples in NLP: A Survey (arXiv)](https://arxiv.org/abs/1812.05271) - Comprehensive survey of attacks and defenses in NLP.
- [A Survey on Robustness Evaluation for Natural Language Processing (arXiv)](https://arxiv.org/abs/2009.07012) - Systematic review of robustness evaluation techniques for NLP models.
- [On Evaluating Adversarial Robustness (ICLR)](https://arxiv.org/abs/1902.06705) - Discussion of pitfalls and best practices in robustness evaluation.

## Videos and Presentations

- [Tutorials on Model Robustness — Conference Talks (YouTube search)](https://www.youtube.com/results?search_query=model+robustness+tutorial) - Conference tutorials and talks covering practical robustness evaluation methods.
- [Adversarial NLP Workshop Talks (various conferences)](https://www.youtube.com/results?search_query=adversarial+nlp+workshop) - Recorded presentations on attack/defense techniques and evaluations.

## Open‑Source Resources

- [GLUE / SuperGLUE Benchmarks](https://gluebenchmark.com) - Standard language understanding benchmarks for baseline evaluation.
- [Papers with Code — Adversarial Robustness](https://paperswithcode.com/task/adversarial-robustness) - Curated papers, implementations, and benchmarks related to robustness.
- [Robustness Benchmarking Repositories (various)](https://paperswithcode.com) - Collections of tasks and code to reproduce robustness results.
